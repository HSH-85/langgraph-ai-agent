"""
LangGraph RAG ì—ì´ì „íŠ¸ì˜ ë…¸ë“œ í•¨ìˆ˜ë“¤
"""
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.messages import HumanMessage, AIMessage
from tavily import TavilyClient
from state import AgentState
import cohere
import os
from datetime import datetime
from dotenv import load_dotenv

# í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€
GraphState = AgentState

load_dotenv()

# API í‚¤ í™•ì¸
openai_api_key = os.getenv("OPENAI_API_KEY")
tavily_api_key = os.getenv("TAVILY_API_KEY")

# API í‚¤ í™•ì¸ (ì¡°ìš©í•œ ëª¨ë“œ - í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
# if not openai_api_key:
#     print("âš ï¸ ê²½ê³ : OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
#     print("   .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

# LLM ì´ˆê¸°í™” (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ìë™ìœ¼ë¡œ API í‚¤ë¥¼ ì½ì–´ì˜´)
# api_key íŒŒë¼ë¯¸í„°ë¥¼ ëª…ì‹œí•˜ì§€ ì•Šìœ¼ë©´ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ì‚¬ìš©
llm = ChatOpenAI(
    model="gpt-4o-mini", 
    temperature=0
)
embeddings = OpenAIEmbeddings()

# Tavily í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
if tavily_api_key:
    tavily_client = TavilyClient(api_key=tavily_api_key)
else:
    tavily_client = None
    # print("âš ï¸ ê²½ê³ : TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# Cohere í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë¦¬ë­í¬ìš©)
cohere_api_key = os.getenv("COHERE_API_KEY")
if cohere_api_key:
    cohere_client = cohere.Client(api_key=cohere_api_key)
else:
    cohere_client = None
    # print("âš ï¸ ê²½ê³ : COHERE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¦¬ë­í¬ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


# ì „ì—­ ë³€ìˆ˜: Retriever (ì´ˆê¸°í™” í•„ìš”)
retriever = None


def set_retriever(new_retriever):
    """Retrieverë¥¼ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜"""
    global retriever
    retriever = new_retriever


def analyze_intent(state: AgentState) -> AgentState:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ëŠ” ë…¸ë“œ (ê³ ë„í™”)
    
    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (intent, current_step, thought_process)
    """
    question = state.get("question", "")
    thought_process = state.get("thought_process", [])
    
    # ì˜ë„ ë¶„ì„ í”„ë¡¬í”„íŠ¸
    intent_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ì˜ë„ ë¶„ë¥˜:
1. 'factual': ì‚¬ì‹¤ì  ì •ë³´ë‚˜ ì§€ì‹ì„ ë¬»ëŠ” ì§ˆë¬¸ (ì˜ˆ: "íŒŒì´ì¬ì´ë€?", "LangGraph íŠ¹ì§•ì€?")
2. 'analytical': ë¹„êµ, ë¶„ì„, í‰ê°€ë¥¼ ìš”êµ¬í•˜ëŠ” ì§ˆë¬¸ (ì˜ˆ: "Aì™€ Bì˜ ì°¨ì´ëŠ”?", "ì¥ë‹¨ì ì€?")
3. 'conversational': ì¼ë°˜ ëŒ€í™”ë‚˜ ì˜ê²¬ êµí™˜ (ì˜ˆ: "ì•ˆë…•", "ê³ ë§ˆì›Œ")
4. 'procedural': ì ˆì°¨, ë°©ë²•, ì‚¬ìš©ë²•ì„ ë¬»ëŠ” ì§ˆë¬¸ (ì˜ˆ: "ì–´ë–»ê²Œ í•˜ë‚˜ìš”?", "ì„¤ì¹˜ ë°©ë²•ì€?")

ìœ„ 4ê°€ì§€ ì¤‘ í•˜ë‚˜ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”: """

    try:
        response = llm.invoke(intent_prompt)
        intent = response.content.strip().lower() if hasattr(response, 'content') else "factual"
        
        # ìœ íš¨í•œ ì˜ë„ ê°’ìœ¼ë¡œ ì •ê·œí™”
        valid_intents = ['factual', 'analytical', 'conversational', 'procedural']
        if intent not in valid_intents:
            intent = 'factual'  # ê¸°ë³¸ê°’
            
        thought_process.append(f"ğŸ§  ì˜ë„ ë¶„ì„: {intent}")
    except Exception:
        intent = 'factual'
        thought_process.append("ğŸ§  ì˜ë„ ë¶„ì„: factual (ê¸°ë³¸ê°’)")
    
    return {
        "intent": intent,
        "current_step": "ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...",
        "thought_process": thought_process
    }


def retrieve(state: AgentState) -> AgentState:
    """
    ì§ˆë¬¸ì„ ë°›ì•„ VectorDBì—ì„œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë…¸ë“œ
    
    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (documents í¬í•¨)
    """
    question = state.get("question", "")
    documents = state.get("documents", [])
    thought_process = state.get("thought_process", [])
    
    # Retrieverê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ChromaDBì—ì„œ ì§ì ‘ ê²€ìƒ‰
    if retriever is None:
        try:
            vectorstore = Chroma(
                persist_directory="./chroma_db",
                embedding_function=embeddings
            )
            docs = vectorstore.similarity_search(question, k=5)  # ë¦¬ë­í¬ë¥¼ ìœ„í•´ ë” ë§ì´ ê²€ìƒ‰
            documents = [doc.page_content for doc in docs]
            thought_process.append(f"ğŸ“š ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰: {len(documents)}ê°œ ë¬¸ì„œ ë°œê²¬")
        except Exception:
            # ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì¡°ìš©íˆ ì²˜ë¦¬)
            documents = []
            thought_process.append("ğŸ“š ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰: ë¬¸ì„œ ì—†ìŒ")
    else:
        # Retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰
        try:
            docs = retriever.invoke(question)
            documents = [doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in docs]
            thought_process.append(f"ğŸ“š ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰: {len(documents)}ê°œ ë¬¸ì„œ ë°œê²¬")
        except Exception:
            # Retriever ê²€ìƒ‰ ì˜¤ë¥˜ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì¡°ìš©íˆ ì²˜ë¦¬)
            documents = []
            thought_process.append("ğŸ“š ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰: ë¬¸ì„œ ì—†ìŒ")
    
    return {
        "documents": documents,
        "current_step": "ë¬¸ì„œ ë¦¬ë­í¬ ì¤‘...",
        "thought_process": thought_process
    }


def rerank_documents(state: AgentState) -> AgentState:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ë¦¬ë­í¬í•˜ì—¬ ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì¬ì •ë ¬í•˜ëŠ” ë…¸ë“œ (ê³ ë„í™”)
    
    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (ì¬ì •ë ¬ëœ documents)
    """
    question = state.get("question", "")
    documents = state.get("documents", [])
    thought_process = state.get("thought_process", [])
    
    # ë¬¸ì„œê°€ ì—†ê±°ë‚˜ Cohere í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if not documents or len(documents) == 0:
        thought_process.append("ğŸ¯ ë¦¬ë­í¬: ë¬¸ì„œ ì—†ìŒ")
        return {
            "documents": documents,
            "current_step": "ë¬¸ì„œ í‰ê°€ ì¤‘...",
            "thought_process": thought_process
        }
    
    if cohere_client is None:
        thought_process.append(f"ğŸ¯ ë¦¬ë­í¬: ìŠ¤í‚µ (Cohere ë¯¸ì„¤ì •, {len(documents)}ê°œ ìœ ì§€)")
        return {
            "documents": documents,
            "current_step": "ë¬¸ì„œ í‰ê°€ ì¤‘...",
            "thought_process": thought_process
        }
    
    try:
        # Cohere Rerank APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì¬ì •ë ¬
        # top_n: ìƒìœ„ Nê°œ ë¬¸ì„œë§Œ ë°˜í™˜ (ê¸°ë³¸ê°’: ë¬¸ì„œ ì „ì²´)
        rerank_response = cohere_client.rerank(
            model="rerank-multilingual-v3.0",  # ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸
            query=question,
            documents=documents,
            top_n=min(len(documents), 3)  # ìµœëŒ€ 3ê°œ ë¬¸ì„œë§Œ ë°˜í™˜
        )
        
        # ì¬ì •ë ¬ëœ ë¬¸ì„œ ì¶”ì¶œ
        # documentsê°€ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ result.documentë„ ë¬¸ìì—´ì…ë‹ˆë‹¤
        reranked_documents = [
            result.document['text'] if isinstance(result.document, dict) and 'text' in result.document
            else str(result.document) if not isinstance(result.document, str)
            else result.document
            for result in rerank_response.results
        ]
        
        thought_process.append(f"ğŸ¯ ë¦¬ë­í¬: {len(documents)}ê°œ â†’ {len(reranked_documents)}ê°œ (ìƒìœ„ ë¬¸ì„œ ì„ íƒ)")
        
        return {
            "documents": reranked_documents,
            "current_step": "ë¬¸ì„œ í‰ê°€ ì¤‘...",
            "thought_process": thought_process
        }
    except Exception as e:
        # ë¦¬ë­í¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë¬¸ì„œ ë°˜í™˜
        thought_process.append(f"ğŸ¯ ë¦¬ë­í¬: ì‹¤íŒ¨ ({len(documents)}ê°œ ìœ ì§€)")
        return {
            "documents": documents,
            "current_step": "ë¬¸ì„œ í‰ê°€ ì¤‘...",
            "thought_process": thought_process
        }


def grade_documents(state: AgentState) -> AgentState:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ”ì§€ LLM(gpt-4o-mini)ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë…¸ë“œ (ê³ ë„í™”)
    ê´€ë ¨ ì—†ìœ¼ë©´ web_search=True ì„¤ì •
    
    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (web_search, is_relevant, loop_count í¬í•¨)
    """
    question = state.get("question", "")
    documents = state.get("documents", [])
    thought_process = state.get("thought_process", [])
    loop_count = state.get("loop_count", 0)
    
    # ë¬´í•œ ë£¨í”„ ë°©ì§€: ê¸ˆìœµ íŠ¹í™” ìµœëŒ€ 7íšŒê¹Œì§€ë§Œ ì¬ì‹œë„
    if loop_count >= 7:
        thought_process.append(f"âš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬ ({loop_count}íšŒ)")
        return {
            "web_search": False,
            "is_relevant": "no",
            "loop_count": loop_count,
            "current_step": "ë‹µë³€ ìƒì„± ì¤‘...",
            "thought_process": thought_process
        }
    
    # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì›¹ ê²€ìƒ‰ í•„ìš”
    if not documents or len(documents) == 0:
        thought_process.append("âŒ ë¬¸ì„œ í‰ê°€: ë¬¸ì„œ ì—†ìŒ â†’ ì›¹ ê²€ìƒ‰ í•„ìš”")
        return {
            "web_search": True,
            "is_relevant": "no",
            "loop_count": loop_count + 1,
            "current_step": "ì›¹ ê²€ìƒ‰ ì¤‘...",
            "thought_process": thought_process
        }
    
    # ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
    documents_text = "\n\n".join([f"[ë¬¸ì„œ {i+1}]\n{doc}" for i, doc in enumerate(documents)])
    
    # LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€
    evaluation_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ê²€í† í•˜ê³ , ë¬¸ì„œë“¤ì´ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°ì— ì¶©ë¶„í•œ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:
{documents_text}

í‰ê°€ ê¸°ì¤€:
- ë¬¸ì„œë“¤ì´ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆê³  ë‹µë³€ì— ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²½ìš°: "yes"
- ë¬¸ì„œë“¤ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì—†ê±°ë‚˜ ë‹µë³€ì— í•„ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš°: "no"

ë‹µë³€ì€ ë°˜ë“œì‹œ "yes" ë˜ëŠ” "no"ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

    try:
        response = llm.invoke(evaluation_prompt)
        evaluation = response.content.strip().lower() if hasattr(response, 'content') else str(response).strip().lower()
        
        # "yes"ê°€ ì•„ë‹ˆë©´ ì›¹ ê²€ìƒ‰ í•„ìš”
        web_search_needed = not (evaluation.startswith("yes") or evaluation == "yes")
        is_relevant = "yes" if not web_search_needed else "no"
        
        if web_search_needed:
            thought_process.append(f"âŒ ë¬¸ì„œ í‰ê°€: ê´€ë ¨ì„± ë‚®ìŒ â†’ ì›¹ ê²€ìƒ‰ í•„ìš” ({loop_count + 1}íšŒ ì‹œë„)")
        else:
            thought_process.append("âœ… ë¬¸ì„œ í‰ê°€: ê´€ë ¨ì„± ë†’ìŒ â†’ ë‹µë³€ ìƒì„±")
            
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í´ë°± (ì¡°ìš©íˆ ì²˜ë¦¬)
        web_search_needed = True
        is_relevant = "no"
        thought_process.append(f"âš ï¸ ë¬¸ì„œ í‰ê°€: ì˜¤ë¥˜ ë°œìƒ â†’ ì›¹ ê²€ìƒ‰ ì‹¤í–‰ ({loop_count + 1}íšŒ ì‹œë„)")
    
    return {
        "web_search": web_search_needed,
        "is_relevant": is_relevant,
        "loop_count": loop_count + 1 if web_search_needed else loop_count,
        "current_step": "ì›¹ ê²€ìƒ‰ ì¤‘..." if web_search_needed else "ë‹µë³€ ìƒì„± ì¤‘...",
        "thought_process": thought_process
    }


def web_search_node(state: AgentState) -> AgentState:
    """
    web_search=Trueì¼ ë•Œ Tavily APIë¡œ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¬¸ì„œì— ì¶”ê°€í•˜ëŠ” ë…¸ë“œ (ê³ ë„í™”)
    í˜„ì¬ ì‹œê°„ ì •ë³´ë¥¼ ê²€ìƒ‰ ì¿¼ë¦¬ì— í¬í•¨í•˜ì—¬ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (documentsì— ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€)
    """
    question = state.get("question", "")
    existing_documents = state.get("documents", [])
    thought_process = state.get("thought_process", [])
    search_round = state.get("search_round", 0)
    financial_domain = state.get("financial_domain", "general")
    
    if tavily_client is None:
        thought_process.append("âš ï¸ ì›¹ ê²€ìƒ‰: Tavily ë¯¸ì„¤ì • â†’ ìŠ¤í‚µ")
        return {
            "documents": existing_documents,
            "web_search": False,
            "current_step": "ë‹µë³€ ìƒì„± ì¤‘...",
            "thought_process": thought_process
        }
    
    try:
        # í˜„ì¬ ì‹œê°„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        now = datetime.now()
        current_date = now.strftime("%Yë…„ %mì›” %dì¼")
        current_time = now.strftime("%Hì‹œ %Më¶„")
        current_datetime_str = f"{current_date} {current_time}"
        
        # ë‹¤ì¤‘ ê²€ìƒ‰ ë¼ìš´ë“œ: ê° ë¼ìš´ë“œë§ˆë‹¤ ë‹¤ë¥¸ ì¿¼ë¦¬ ìƒì„±
        domain_keywords = {
            "stock": ["ì£¼ê°€", "ì‹œê°€ì´ì•¡", "PER", "PBR", "ê¸°ì—… ì‹¤ì "],
            "bond": ["ìˆ˜ìµë¥ ", "ë§Œê¸°", "ì‹ ìš©ë“±ê¸‰", "ì´ììœ¨"],
            "forex": ["í™˜ìœ¨", "í™˜ì°¨ìµ", "í†µí™”ì •ì±…"],
            "real_estate": ["ë¶€ë™ì‚° ê°€ê²©", "ì „ì„¸", "ì›”ì„¸", "ë¶€ë™ì‚° ì‹œì¥"],
            "interest_rate": ["ê¸ˆë¦¬", "ê¸°ì¤€ê¸ˆë¦¬", "ê¸ˆë¦¬ ì •ì±…"],
            "derivative": ["íŒŒìƒìƒí’ˆ", "ì˜µì…˜", "ì„ ë¬¼"],
            "crypto": ["ì•”í˜¸í™”í", "ë¹„íŠ¸ì½”ì¸", "ê°€ìƒìì‚°"],
            "economic": ["ê²½ì œ ì§€í‘œ", "GDP", "ì¸í”Œë ˆì´ì…˜"],
        }
        
        # ê²€ìƒ‰ ë¼ìš´ë“œë³„ ì¿¼ë¦¬ ìƒì„±
        if search_round == 0:
            enhanced_query = f"{question} (í˜„ì¬ ì‹œê°„: {current_datetime_str})"
        elif search_round == 1:
            keywords = domain_keywords.get(financial_domain, [])
            additional = f" {keywords[0]}" if keywords else ""
            enhanced_query = f"{question}{additional} ìµœì‹  ë™í–¥ (í˜„ì¬ ì‹œê°„: {current_datetime_str})"
        elif search_round == 2:
            # 3ë¼ìš´ë“œ: ì—…ì¢…/ê²½ìŸì‚¬ ë¹„êµ ì •ë³´ ê²€ìƒ‰ (ì£¼ì‹ ë„ë©”ì¸ì¸ ê²½ìš°)
            if financial_domain == "stock":
                enhanced_query = f"{question} ì—…ì¢… ê²½ìŸì‚¬ ë¹„êµ ë¶„ì„ (í˜„ì¬ ì‹œê°„: {current_datetime_str})"
            else:
                enhanced_query = f"{question} ìƒì„¸ ë¶„ì„ ìµœì‹  ì •ë³´ (í˜„ì¬ ì‹œê°„: {current_datetime_str})"
        else:
            enhanced_query = f"{question} ìƒì„¸ ë¶„ì„ ìµœì‹  ì •ë³´ (í˜„ì¬ ì‹œê°„: {current_datetime_str})"
        
        # Tavily ê²€ìƒ‰
        response = tavily_client.search(
            query=enhanced_query,
            max_results=3,
            search_depth="advanced"
        )
        
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ì„œì— ì¶”ê°€ (í˜„ì¬ ì‹œê°„ ì •ë³´ í¬í•¨)
        search_results = []
        for result in response.get("results", []):
            content = result.get("content", "")
            if content:
                # ê° ê²€ìƒ‰ ê²°ê³¼ì— í˜„ì¬ ì‹œê°„ ì •ë³´ ì¶”ê°€
                timestamped_content = f"[ê²€ìƒ‰ ì‹œê°„: {current_datetime_str}, ê²€ìƒ‰ ë¼ìš´ë“œ: {search_round + 1}]\n{content}"
                search_results.append(timestamped_content)
        
        existing_documents.extend(search_results)
        thought_process.append(f"ğŸŒ ì›¹ ê²€ìƒ‰ ({search_round + 1}ë¼ìš´ë“œ): {len(search_results)}ê°œ ê²°ê³¼ ì¶”ê°€ (ê²€ìƒ‰ ì‹œê°„: {current_datetime_str})")
    except Exception as e:
        # ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜ ì‹œ ê¸°ì¡´ ë¬¸ì„œ ìœ ì§€ (ì¡°ìš©íˆ ì²˜ë¦¬)
        thought_process.append("âš ï¸ ì›¹ ê²€ìƒ‰: ì˜¤ë¥˜ ë°œìƒ")
    
    return {
        "documents": existing_documents,
        "search_round": search_round + 1,  # ê²€ìƒ‰ ë¼ìš´ë“œ ì¦ê°€
        "web_search": False,  # ì›¹ ê²€ìƒ‰ ì™„ë£Œ í›„ í”Œë˜ê·¸ ë¦¬ì…‹
        "current_step": "í¬ë¡œìŠ¤ ê²€ì¦ ì¤‘...",
        "thought_process": thought_process
    }


def generate(state: AgentState) -> AgentState:
    """
    í™•ë³´ëœ ë¬¸ì„œë“¤ì„ contextë¡œ ì‚¼ì•„ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ (ê³ ë„í™”)
    í˜„ì¬ ì‹œê°„ ì •ë³´ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•˜ì—¬ ì •í™•í•œ ì‹œê°„ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.
    
    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (messages, contextì— ë‹µë³€ ì¶”ê°€)
    """
    question = state.get("question", "")
    documents = state.get("documents", [])
    messages = state.get("messages", [])
    intent = state.get("intent", "factual")
    thought_process = state.get("thought_process", [])
    
    # í˜„ì¬ ì‹œê°„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    now = datetime.now()
    current_date = now.strftime("%Yë…„ %mì›” %dì¼")
    current_time = now.strftime("%Hì‹œ %Më¶„")
    current_datetime_str = f"{current_date} {current_time}"
    current_weekday = now.strftime("%A")  # ì˜ì–´ ìš”ì¼
    weekday_kr = {
        'Monday': 'ì›”ìš”ì¼',
        'Tuesday': 'í™”ìš”ì¼',
        'Wednesday': 'ìˆ˜ìš”ì¼',
        'Thursday': 'ëª©ìš”ì¼',
        'Friday': 'ê¸ˆìš”ì¼',
        'Saturday': 'í† ìš”ì¼',
        'Sunday': 'ì¼ìš”ì¼'
    }
    current_weekday_kr = weekday_kr.get(current_weekday, current_weekday)
    full_datetime_str = f"{current_date} {current_weekday_kr} {current_time}"
    
    # ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    if documents:
        context_text = "\n\n".join([f"[ë¬¸ì„œ {i+1}]\n{doc}" for i, doc in enumerate(documents)])
        thought_process.append(f"ğŸ“ ë‹µë³€ ìƒì„±: {len(documents)}ê°œ ë¬¸ì„œ ê¸°ë°˜")
    else:
        context_text = "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        thought_process.append("ğŸ“ ë‹µë³€ ìƒì„±: ë¬¸ì„œ ì—†ìŒ (ì¼ë°˜ ì‘ë‹µ)")
    
    # ì˜ë„ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
    intent_instructions = {
        'factual': "ì •í™•í•œ ì‚¬ì‹¤ì„ ì œê³µí•˜ê³ , ì¶œì²˜ê°€ ëª…í™•í•œ ì •ë³´ë¥¼ ìš°ì„ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.",
        'analytical': "ë¹„êµ, ë¶„ì„, í‰ê°€ë¥¼ í†µí•´ ë‹¤ê°ë„ë¡œ ë‹µë³€í•˜ê³ , ì¥ë‹¨ì ì„ ê· í˜•ìˆê²Œ ì œì‹œí•˜ì„¸ìš”.",
        'conversational': "ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ ì–´ì¡°ë¡œ ë‹µë³€í•˜ì„¸ìš”.",
        'procedural': "ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ê³ , ì‹¤í–‰ ê°€ëŠ¥í•œ ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”."
    }
    intent_instruction = intent_instructions.get(intent, intent_instructions['factual'])
    
    # ì´ì „ ëŒ€í™” ë§¥ë½ êµ¬ì„± (ìµœê·¼ 3ê°œ ëŒ€í™”ë§Œ í¬í•¨í•˜ì—¬ í† í° ì ˆì•½)
    conversation_context = ""
    if messages and len(messages) > 0:
        # ìµœê·¼ ëŒ€í™”ë§Œ ì¶”ì¶œ (HumanMessage, AIMessage ìŒ)
        recent_messages = messages[-6:] if len(messages) > 6 else messages
        conversation_parts = []
        for msg in recent_messages:
            if hasattr(msg, 'content'):
                role = "ì‚¬ìš©ì" if hasattr(msg, '__class__') and "Human" in msg.__class__.__name__ else "ì–´ì‹œìŠ¤í„´íŠ¸"
                conversation_parts.append(f"[{role}]: {msg.content}")
        if conversation_parts:
            conversation_context = "\n\n[ì´ì „ ëŒ€í™” ë§¥ë½]\n" + "\n".join(conversation_parts) + "\n\n"
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (í˜„ì¬ ì‹œê°„ ì •ë³´ ë° ì´ì „ ëŒ€í™” ë§¥ë½ í¬í•¨)
    prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê³  ì—°ê´€ì„± ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

[ì¤‘ìš”: í˜„ì¬ ì‹œê°„ ì •ë³´]
í˜„ì¬ ì‹œê°„ì€ {full_datetime_str}ì…ë‹ˆë‹¤.
ì‹œê°„ ê´€ë ¨ ì§ˆë¬¸ì´ ìˆë‹¤ë©´ ì´ ì •ë³´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

{conversation_context}
[ë‹µë³€ ìŠ¤íƒ€ì¼]
{intent_instruction}

[ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]
{context_text}

[í˜„ì¬ ì§ˆë¬¸]
{question}

[ë‹µë³€] (ì´ì „ ëŒ€í™”ì™€ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ë„ë¡ ë‹µë³€í•˜ì„¸ìš”)
"""

    # LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
    try:
        response = llm.invoke(prompt)
        answer = response.content if hasattr(response, 'content') else str(response)
        thought_process.append("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
    except Exception as e:
        error_msg = str(e)
        
        # ë” ìì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€ ì œê³µ
        if "insufficient_quota" in error_msg or "429" in error_msg:
            answer = """âš ï¸ OpenAI API í• ë‹¹ëŸ‰ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

ê°€ëŠ¥í•œ ì›ì¸:
1. OpenAI ê³„ì •ì— í¬ë ˆë”§ì´ ì—†ìŠµë‹ˆë‹¤
   â†’ https://platform.openai.com/account/billing ì—ì„œ í¬ë ˆë”§ì„ í™•ì¸í•˜ì„¸ìš”
2. ë¬´ë£Œ í‹°ì–´ì˜ ê²½ìš° ì´ˆê¸° í¬ë ˆë”§ì´ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
   â†’ ê²°ì œ ìˆ˜ë‹¨ì„ ì¶”ê°€í•˜ê±°ë‚˜ í¬ë ˆë”§ì„ ì¶©ì „í•˜ì„¸ìš”
3. API í‚¤ê°€ ì˜ëª»ë˜ì—ˆê±°ë‚˜ ë‹¤ë¥¸ ê³„ì •ì˜ í‚¤ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤
   â†’ .env íŒŒì¼ì˜ OPENAI_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”"""
        elif "api_key" in error_msg.lower():
            answer = """âš ï¸ OpenAI API í‚¤ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

.env íŒŒì¼ì— ì˜¬ë°”ë¥¸ OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.
API í‚¤ëŠ” https://platform.openai.com/api-keys ì—ì„œ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
        else:
            answer = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\nì˜¤ë¥˜ ë‚´ìš©: {error_msg}"
    
    # ë©”ì‹œì§€ì— ì¶”ê°€
    updated_messages = messages + [
        HumanMessage(content=question),
        AIMessage(content=answer)
    ]
    
    return {
        "messages": updated_messages,
        "context": context_text,
        "current_step": "ì™„ë£Œ",
        "thought_process": thought_process
    }


def analyze_financial_domain(state: AgentState) -> AgentState:
    """
    ê¸ˆìœµ ë„ë©”ì¸ ë¶„ì„ ë…¸ë“œ
    ì§ˆë¬¸ì„ ê¸ˆìœµ ë„ë©”ì¸ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤ (ì£¼ì‹, ì±„ê¶Œ, ì™¸í™˜, ë¶€ë™ì‚° ë“±)
    
    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (financial_domain í¬í•¨)
    """
    question = state.get("question", "")
    thought_process = state.get("thought_process", [])
    
    financial_domain_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì„ ê¸ˆìœµ ë„ë©”ì¸ìœ¼ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ê¸ˆìœµ ë„ë©”ì¸ ë¶„ë¥˜ (í•˜ë‚˜ë§Œ ì„ íƒ):
- 'stock': ì£¼ì‹, ì£¼ê°€, ê¸°ì—… ë¶„ì„, ì‹œê°€ì´ì•¡, PER, PBR, ë°°ë‹¹ ë“± (íšŒì‚¬ëª… + ì£¼ê°€/ì£¼ì‹ ê´€ë ¨ ì§ˆë¬¸ì€ ë°˜ë“œì‹œ stock ì„ íƒ)
- 'bond': ì±„ê¶Œ, ì´ììœ¨, ìˆ˜ìµë¥ , ë§Œê¸°, ì‹ ìš©ë“±ê¸‰ ë“±
- 'forex': ì™¸í™˜, í™˜ìœ¨, í†µí™”ì •ì±…, í™˜ì°¨ìµ ë“±
- 'real_estate': ë¶€ë™ì‚°, ì§‘ê°’, ì „ì„¸, ì›”ì„¸, ë¶€ë™ì‚° íˆ¬ì ë“±
- 'interest_rate': ê¸ˆë¦¬, ê¸°ì¤€ê¸ˆë¦¬, ê¸ˆë¦¬ ì •ì±… ë“±
- 'derivative': íŒŒìƒìƒí’ˆ, ì˜µì…˜, ì„ ë¬¼, ìŠ¤ì™‘ ë“±
- 'crypto': ì•”í˜¸í™”í, ê°€ìƒìì‚°, ë¹„íŠ¸ì½”ì¸, ì´ë”ë¦¬ì›€ ë“±
- 'economic': ê²½ì œ ì§€í‘œ, GDP, ì¸í”Œë ˆì´ì…˜, ì‹¤ì—…ë¥ , ê²½ê¸° ë“±
- 'general': ì¼ë°˜ ê¸ˆìœµ, ê¸ˆìœµ ìƒí’ˆ, ê¸ˆìœµ ì„œë¹„ìŠ¤ ë“±

ì¤‘ìš”: íšŒì‚¬ëª…ì´ í¬í•¨ëœ ì£¼ê°€/ì£¼ì‹ ê´€ë ¨ ì§ˆë¬¸ì€ ë°˜ë“œì‹œ 'stock'ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
ì˜ˆ: "ì‚¼ì„±ì „ì ì£¼ê°€ëŠ”?" â†’ stock
ì˜ˆ: "ì• í”Œ ì£¼ê°€" â†’ stock
ì˜ˆ: "SKí•˜ì´ë‹‰ìŠ¤ ì£¼ì‹" â†’ stock

ìœ„ ëª©ë¡ ì¤‘ í•˜ë‚˜ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”: """
    
    try:
        response = llm.invoke(financial_domain_prompt)
        domain = response.content.strip().lower() if hasattr(response, 'content') else "general"
        
        # ìœ íš¨í•œ ë„ë©”ì¸ ê°’ìœ¼ë¡œ ì •ê·œí™”
        valid_domains = ['stock', 'bond', 'forex', 'real_estate', 'interest_rate', 
                        'derivative', 'crypto', 'economic', 'general']
        if domain not in valid_domains:
            domain = 'general'
        
        domain_kr = {
            'stock': 'ì£¼ì‹',
            'bond': 'ì±„ê¶Œ',
            'forex': 'ì™¸í™˜',
            'real_estate': 'ë¶€ë™ì‚°',
            'interest_rate': 'ê¸ˆë¦¬',
            'derivative': 'íŒŒìƒìƒí’ˆ',
            'crypto': 'ì•”í˜¸í™”í',
            'economic': 'ê²½ì œ ì§€í‘œ',
            'general': 'ì¼ë°˜ ê¸ˆìœµ'
        }
        
        thought_process.append(f"ğŸ’° ê¸ˆìœµ ë„ë©”ì¸ ë¶„ì„: {domain_kr.get(domain, domain)}")
    except Exception:
        domain = 'general'
        thought_process.append("ğŸ’° ê¸ˆìœµ ë„ë©”ì¸ ë¶„ì„: ì¼ë°˜ ê¸ˆìœµ (ê¸°ë³¸ê°’)")
    
    return {
        "financial_domain": domain,
        "current_step": "ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...",
        "thought_process": thought_process
    }


def verify_documents(state: AgentState) -> AgentState:
    """
    ê¸ˆìœµ ë¬¸ì„œ ê²€ì¦ ë…¸ë“œ (2ì°¨ ê²€ì¦)
    ë°ì´í„° ì¼ê´€ì„±, ì¶œì²˜ ì‹ ë¢°ì„±, ì‹œì  ì ì ˆì„±, ìƒì¶© ì •ë³´ í™•ì¸
    
    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (verification_round ì¦ê°€)
    """
    question = state.get("question", "")
    documents = state.get("documents", [])
    thought_process = state.get("thought_process", [])
    verification_round = state.get("verification_round", 0)
    
    if not documents or len(documents) == 0:
        thought_process.append("ğŸ” ë¬¸ì„œ ê²€ì¦: ë¬¸ì„œ ì—†ìŒ")
        return {
            "verification_round": verification_round + 1,
            "thought_process": thought_process
        }
    
    documents_text = "\n\n".join([f"[ë¬¸ì„œ {i+1}]\n{doc[:1000]}..." if len(doc) > 1000 else f"[ë¬¸ì„œ {i+1}]\n{doc}" 
                                  for i, doc in enumerate(documents)])
    
    verification_prompt = f"""ë‹¤ìŒ ê¸ˆìœµ ê´€ë ¨ ì§ˆë¬¸ê³¼ ë¬¸ì„œë“¤ì„ ê²€ì¦í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {question}

ë¬¸ì„œë“¤:
{documents_text}

ê²€ì¦ í•­ëª©:
1. ë°ì´í„° ì¼ê´€ì„±: ìˆ«ì, ë‚ ì§œ, í†µê³„, ë¹„ìœ¨ì´ ë¬¸ì„œ ê°„ ì¼ì¹˜í•˜ëŠ”ê°€?
2. ì¶œì²˜ ì‹ ë¢°ì„±: ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê¸ˆìœµ ê¸°ê´€/ë¯¸ë””ì–´/ë°ì´í„° ì†ŒìŠ¤ì¸ê°€?
3. ì‹œì  ì ì ˆì„±: ìµœì‹  ì •ë³´ì¸ê°€? (ê¸ˆìœµ ì‹œì¥ì€ ë¹ ë¥´ê²Œ ë³€í™”í•¨)
4. ìƒì¶© ì •ë³´: ë¬¸ì„œ ê°„ ëª¨ìˆœì´ë‚˜ ì¶©ëŒí•˜ëŠ” ì •ë³´ê°€ ìˆëŠ”ê°€?

ê° ë¬¸ì„œì— ëŒ€í•´ í‰ê°€í•˜ê³ , ì „ì²´ì ìœ¼ë¡œ:
- "verified": ê²€ì¦ ì™„ë£Œ, ì‹ ë¢°í•  ìˆ˜ ìˆìŒ
- "needs_cross_check": ì¶”ê°€ í™•ì¸ í•„ìš”
- "unreliable": ì‹ ë¢°í•  ìˆ˜ ì—†ìŒ

ê²€ì¦ ê²°ê³¼ë¥¼ í•œ ë‹¨ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”: """
    
    try:
        response = llm.invoke(verification_prompt)
        result = response.content.strip().lower() if hasattr(response, 'content') else "needs_cross_check"
        
        if "verified" in result or "ê²€ì¦ ì™„ë£Œ" in result:
            thought_process.append(f"âœ… ë¬¸ì„œ ê²€ì¦ ({verification_round + 1}ë¼ìš´ë“œ): ê²€ì¦ ì™„ë£Œ")
        elif "unreliable" in result or "ì‹ ë¢°í•  ìˆ˜ ì—†" in result:
            thought_process.append(f"âš ï¸ ë¬¸ì„œ ê²€ì¦ ({verification_round + 1}ë¼ìš´ë“œ): ì‹ ë¢°ë„ ë‚®ìŒ")
        else:
            thought_process.append(f"ğŸ”„ ë¬¸ì„œ ê²€ì¦ ({verification_round + 1}ë¼ìš´ë“œ): ì¶”ê°€ í™•ì¸ í•„ìš”")
    except Exception:
        thought_process.append(f"âš ï¸ ë¬¸ì„œ ê²€ì¦ ({verification_round + 1}ë¼ìš´ë“œ): ì˜¤ë¥˜ ë°œìƒ")
    
    return {
        "verification_round": verification_round + 1,
        "current_step": "ë¬¸ì„œ ì¬í‰ê°€ ì¤‘...",
        "thought_process": thought_process
    }


def cross_validate(state: AgentState) -> AgentState:
    """
    í¬ë¡œìŠ¤ ê²€ì¦ ë…¸ë“œ: ì—¬ëŸ¬ ì†ŒìŠ¤ ê°„ ì¼ì¹˜ë„ í™•ì¸
    ê¸ˆìœµ ì •ë³´ì˜ ì •í™•ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì—¬ëŸ¬ ì†ŒìŠ¤ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
    
    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (source_agreement, cross_validation_results í¬í•¨)
    """
    documents = state.get("documents", [])
    thought_process = state.get("thought_process", [])
    
    if len(documents) < 2:
        thought_process.append("ğŸ”„ í¬ë¡œìŠ¤ ê²€ì¦: ì†ŒìŠ¤ ë¶€ì¡± (2ê°œ ë¯¸ë§Œ)")
        return {
            "source_agreement": "low",
            "cross_validation_results": [],
            "thought_process": thought_process
        }
    
    # ë¬¸ì„œ ë‚´ìš© ìš”ì•½ (ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸°)
    docs_summary = "\n\n".join([f"[ì†ŒìŠ¤ {i+1}]\n{doc[:800]}..." if len(doc) > 800 else f"[ì†ŒìŠ¤ {i+1}]\n{doc}" 
                                for i, doc in enumerate(documents)])
    
    cross_validation_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì€ ê°™ì€ ê¸ˆìœµ ì§ˆë¬¸ì— ëŒ€í•œ ì—¬ëŸ¬ ì†ŒìŠ¤ì…ë‹ˆë‹¤.
ì†ŒìŠ¤ ê°„ ì¼ì¹˜ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”:

{docs_summary}

í‰ê°€ ê¸°ì¤€:
- í•µì‹¬ ë°ì´í„° (ìˆ«ì, ë¹„ìœ¨, í†µê³„ ë“±)ê°€ ì†ŒìŠ¤ ê°„ ì¼ì¹˜í•˜ëŠ”ê°€?
- ê²°ë¡ ì´ë‚˜ í•´ì„ì´ ì¼ì¹˜í•˜ëŠ”ê°€?
- ì¼ì¹˜í•˜ëŠ” ì •ë³´ì˜ ë¹„ìœ¨ì€ ì–¼ë§ˆì¸ê°€?

ì¼ì¹˜ë„ë¥¼ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ í‰ê°€:
- "high": 80% ì´ìƒ ì¼ì¹˜
- "medium": 50-80% ì¼ì¹˜
- "low": 50% ë¯¸ë§Œ ì¼ì¹˜

ì¼ì¹˜ë„ë§Œ í•œ ë‹¨ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”: """
    
    try:
        response = llm.invoke(cross_validation_prompt)
        agreement = response.content.strip().lower() if hasattr(response, 'content') else "low"
        
        # ì •ê·œí™”
        if "high" in agreement or "ë†’" in agreement or "80" in agreement:
            agreement = "high"
        elif "medium" in agreement or "ì¤‘ê°„" in agreement or "50" in agreement:
            agreement = "medium"
        else:
            agreement = "low"
        
        thought_process.append(f"ğŸ”„ í¬ë¡œìŠ¤ ê²€ì¦: ì†ŒìŠ¤ ì¼ì¹˜ë„ {agreement} ({len(documents)}ê°œ ì†ŒìŠ¤)")
    except Exception:
        agreement = "low"
        thought_process.append("âš ï¸ í¬ë¡œìŠ¤ ê²€ì¦: ì˜¤ë¥˜ ë°œìƒ")
    
    return {
        "source_agreement": agreement,
        "cross_validation_results": [{"agreement": agreement, "source_count": len(documents)}],
        "thought_process": thought_process
    }


def generate_financial(state: AgentState) -> AgentState:
    """
    ê¸ˆìœµ íŠ¹í™” ë‹µë³€ ìƒì„± ë…¸ë“œ
    ê¸ˆìœµ ë„ë©”ì¸ì— íŠ¹í™”ëœ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ë¹„ìŠ·í•œ ì—…ì¢…/ê·œëª¨ì˜ íšŒì‚¬ ë¹„êµ ë¶„ì„ ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
    
    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (messages, contextì— ë‹µë³€ ì¶”ê°€)
    """
    question = state.get("question", "")
    documents = state.get("documents", [])
    messages = state.get("messages", [])
    intent = state.get("intent", "factual")
    financial_domain = state.get("financial_domain", "general")
    confidence_score = state.get("confidence_score", 0.0)
    source_agreement = state.get("source_agreement", "low")
    company_comparison_data = state.get("company_comparison_data")
    thought_process = state.get("thought_process", [])
    
    # í˜„ì¬ ì‹œê°„ ì •ë³´
    now = datetime.now()
    current_date = now.strftime("%Yë…„ %mì›” %dì¼")
    current_time = now.strftime("%Hì‹œ %Më¶„")
    current_weekday = now.strftime("%A")
    weekday_kr = {
        'Monday': 'ì›”ìš”ì¼', 'Tuesday': 'í™”ìš”ì¼', 'Wednesday': 'ìˆ˜ìš”ì¼',
        'Thursday': 'ëª©ìš”ì¼', 'Friday': 'ê¸ˆìš”ì¼', 'Saturday': 'í† ìš”ì¼', 'Sunday': 'ì¼ìš”ì¼'
    }
    current_weekday_kr = weekday_kr.get(current_weekday, current_weekday)
    full_datetime_str = f"{current_date} {current_weekday_kr} {current_time}"
    
    # ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    if documents:
        context_text = "\n\n".join([f"[ë¬¸ì„œ {i+1}]\n{doc}" for i, doc in enumerate(documents)])
        thought_process.append(f"ğŸ“ ê¸ˆìœµ íŠ¹í™” ë‹µë³€ ìƒì„±: {len(documents)}ê°œ ë¬¸ì„œ ê¸°ë°˜")
    else:
        context_text = "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        thought_process.append("ğŸ“ ê¸ˆìœµ íŠ¹í™” ë‹µë³€ ìƒì„±: ë¬¸ì„œ ì—†ìŒ (ì¼ë°˜ ì‘ë‹µ)")
    
    # ì´ì „ ëŒ€í™” ë§¥ë½ êµ¬ì„±
    conversation_context = ""
    if messages and len(messages) > 0:
        recent_messages = messages[-6:] if len(messages) > 6 else messages
        conversation_parts = []
        for msg in recent_messages:
            if hasattr(msg, 'content'):
                role = "ì‚¬ìš©ì" if hasattr(msg, '__class__') and "Human" in msg.__class__.__name__ else "ì–´ì‹œìŠ¤í„´íŠ¸"
                conversation_parts.append(f"[{role}]: {msg.content}")
        if conversation_parts:
            conversation_context = "\n\n[ì´ì „ ëŒ€í™” ë§¥ë½]\n" + "\n".join(conversation_parts) + "\n\n"
    
    # ê¸ˆìœµ ë„ë©”ì¸ë³„ íŠ¹í™” ì§€ì‹œì‚¬í•­
    domain_instructions = {
        'stock': "ì£¼ì‹ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•  ë•ŒëŠ” ì£¼ê°€, ì‹œê°€ì´ì•¡, PER, PBR, ê¸°ì—… ì‹¤ì  ë“±ì„ ì •í™•íˆ í‘œì‹œí•˜ì„¸ìš”.",
        'bond': "ì±„ê¶Œ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•  ë•ŒëŠ” ìˆ˜ìµë¥ , ë§Œê¸°, ì‹ ìš©ë“±ê¸‰, ì´ììœ¨ ë“±ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”.",
        'forex': "ì™¸í™˜ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•  ë•ŒëŠ” í™˜ìœ¨, í†µí™”ì •ì±…, í™˜ì°¨ìµ ë“±ì„ ì‹œì  ì •ë³´ì™€ í•¨ê»˜ ì œê³µí•˜ì„¸ìš”.",
        'real_estate': "ë¶€ë™ì‚° ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•  ë•ŒëŠ” ì§€ì—­, ì‹œê¸°, ê°€ê²© ì¶”ì´ ë“±ì„ ì •í™•íˆ ëª…ì‹œí•˜ì„¸ìš”.",
        'interest_rate': "ê¸ˆë¦¬ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•  ë•ŒëŠ” ê¸°ì¤€ê¸ˆë¦¬, ì‹œì¥ê¸ˆë¦¬, ì •ì±… ê¸ˆë¦¬ ë“±ì„ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.",
        'derivative': "íŒŒìƒìƒí’ˆ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•  ë•ŒëŠ” ë¦¬ìŠ¤í¬ë¥¼ ëª…í™•íˆ ê²½ê³ í•˜ê³  ë³µì¡ì„±ì„ ì„¤ëª…í•˜ì„¸ìš”.",
        'crypto': "ì•”í˜¸í™”í ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•  ë•ŒëŠ” ë³€ë™ì„±ê³¼ ë¦¬ìŠ¤í¬ë¥¼ ê°•ì¡°í•˜ê³  íˆ¬ì ì¡°ì–¸ì´ ì•„ë‹˜ì„ ëª…ì‹œí•˜ì„¸ìš”.",
        'economic': "ê²½ì œ ì§€í‘œ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•  ë•ŒëŠ” ë°ì´í„° ì¶œì²˜, ì‹œì , ë‹¨ìœ„ë¥¼ ì •í™•íˆ í‘œì‹œí•˜ì„¸ìš”.",
        'general': "ê¸ˆìœµ ì •ë³´ë¥¼ ì œê³µí•  ë•ŒëŠ” ì •í™•ì„±ê³¼ ì‹ ë¢°ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ì„¸ìš”."
    }
    domain_instruction = domain_instructions.get(financial_domain, domain_instructions['general'])
    
    # ì‹ ë¢°ë„ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„± í‘œì‹œ
    confidence_note = ""
    if confidence_score < 0.7:
        confidence_note = f"\n[ì¤‘ìš”: ì‹ ë¢°ë„ ì£¼ì˜]\nì´ ë‹µë³€ì˜ ì‹ ë¢°ë„ëŠ” {confidence_score:.2f}ë¡œ ë¹„êµì  ë‚®ìŠµë‹ˆë‹¤. ì†ŒìŠ¤ ê°„ ì¼ì¹˜ë„ê°€ {source_agreement}ì´ë©°, ì¶”ê°€ í™•ì¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    elif confidence_score >= 0.9:
        confidence_note = "\n[ì‹ ë¢°ë„: ë†’ìŒ]\nì—¬ëŸ¬ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ì—ì„œ ì¼ì¹˜í•˜ëŠ” ì •ë³´ì…ë‹ˆë‹¤."
    
    # íšŒì‚¬ ë¹„êµ ë¶„ì„ ì •ë³´ êµ¬ì„±
    comparison_context = ""
    if company_comparison_data and financial_domain == "stock":
        target = company_comparison_data.get("target_company", "")
        industry = company_comparison_data.get("industry", "")
        market_cap = company_comparison_data.get("market_cap_category", "")
        similar = company_comparison_data.get("similar_companies", [])
        insights = company_comparison_data.get("comparison_insights", "")
        
        if target and industry and insights:
            comparison_context = f"""

[ë¹„êµ ë¶„ì„ ì •ë³´]
ëŒ€ìƒ íšŒì‚¬: {target}
ì—…ì¢…: {industry}
ì‹œê°€ì´ì•¡ ê·œëª¨: {market_cap}
ë¹„ìŠ·í•œ ì—…ì¢…/ê·œëª¨ì˜ íšŒì‚¬: {', '.join(similar) if similar else 'ì •ë³´ ì—†ìŒ'}
ë¹„êµ ì¸ì‚¬ì´íŠ¸: {insights}

ìœ„ ë¹„êµ ë¶„ì„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì— ë‹¤ìŒì„ í¬í•¨í•˜ì„¸ìš”:
- ë¹„ìŠ·í•œ ì—…ì¢…/ê·œëª¨ì˜ íšŒì‚¬ë“¤ê³¼ì˜ ë¹„êµ (ì£¼ê°€, ì‹¤ì , ë°¸ë¥˜ì—ì´ì…˜ ì§€í‘œ ë“±)
- ì—…ì¢… í‰ê·  ëŒ€ë¹„ ìœ„ì¹˜ ë¶„ì„
- ë¹„êµë¥¼ í†µí•œ ì¶”ë¡  ë° ì¸ì‚¬ì´íŠ¸
"""
    
    # ê¸ˆìœµ íŠ¹í™” í”„ë¡¬í”„íŠ¸
    prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ê¸ˆìœµ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

[ê¸ˆìœµ ë„ë©”ì¸] {financial_domain}
[í˜„ì¬ ì‹œì ] {full_datetime_str}
[ì‹ ë¢°ë„] {confidence_score:.2f} (ì†ŒìŠ¤ ì¼ì¹˜ë„: {source_agreement})
{confidence_note}

[ê¸ˆìœµ ë‹µë³€ ê°€ì´ë“œë¼ì¸]
1. ì •í™•ì„±: ëª¨ë“  ìˆ«ì, ë¹„ìœ¨, í†µê³„, ë‚ ì§œë¥¼ ì •í™•íˆ ê¸°ì¬
2. ì¶œì²˜ ëª…ì‹œ: ì •ë³´ ì¶œì²˜ë¥¼ ê°€ëŠ¥í•œ í•œ ëª…ì‹œ
3. ë¶ˆí™•ì‹¤ì„± í‘œì‹œ: ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ í•œê³„ê°€ ìˆëŠ” ì •ë³´ëŠ” ëª…í™•íˆ í‘œì‹œ
4. ìµœì‹ ì„±: ì‹œì  ì •ë³´ í¬í•¨ (ì˜ˆ: "2024ë…„ 12ì›” ê¸°ì¤€")
5. ë¦¬ìŠ¤í¬ ê²½ê³ : íˆ¬ì ê´€ë ¨ ì§ˆë¬¸ì€ ë°˜ë“œì‹œ ë¦¬ìŠ¤í¬ ê²½ê³  í¬í•¨
6. ë²•ì  ë©´ì±…: íˆ¬ì ì¡°ì–¸ì´ ì•„ë‹˜ì„ ëª…ì‹œ ("ë³¸ ë‹µë³€ì€ ì •ë³´ ì œê³µ ëª©ì ì´ë©°, íˆ¬ì ì¡°ì–¸ì´ ì•„ë‹™ë‹ˆë‹¤.")
7. ë„ë©”ì¸ íŠ¹í™”: {domain_instruction}

{conversation_context}
[ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]
{context_text}
{comparison_context}
[í˜„ì¬ ì§ˆë¬¸]
{question}

[ë‹µë³€] (ìœ„ ê°€ì´ë“œë¼ì¸ì„ ëª¨ë‘ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ë¹„êµ ë¶„ì„ ì •ë³´ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ì—¬ ë” í’ë¶€í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”.)
"""
    
    try:
        response = llm.invoke(prompt)
        answer = response.content if hasattr(response, 'content') else str(response)
        thought_process.append("âœ… ê¸ˆìœµ íŠ¹í™” ë‹µë³€ ìƒì„± ì™„ë£Œ")
    except Exception as e:
        error_msg = str(e)
        if "insufficient_quota" in error_msg or "429" in error_msg:
            answer = "âš ï¸ OpenAI API í• ë‹¹ëŸ‰ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê³„ì • í¬ë ˆë”§ì„ í™•ì¸í•˜ì„¸ìš”."
        else:
            answer = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\nì˜¤ë¥˜ ë‚´ìš©: {error_msg}"
    
    updated_messages = messages + [
        HumanMessage(content=question),
        AIMessage(content=answer)
    ]
    
    return {
        "messages": updated_messages,
        "context": context_text,
        "current_step": "ì™„ë£Œ",
        "thought_process": thought_process
    }


def extract_and_compare_companies(state: AgentState) -> AgentState:
    """
    íšŒì‚¬ ì •ë³´ ì¶”ì¶œ ë° ë¹„êµ ë¶„ì„ ë…¸ë“œ
    ì§ˆë¬¸ì—ì„œ íšŒì‚¬ëª…ì„ ì¶”ì¶œí•˜ê³ , ë¹„ìŠ·í•œ ì—…ì¢…/ê·œëª¨ì˜ íšŒì‚¬ì™€ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (company_comparison_data í¬í•¨)
    """
    question = state.get("question", "")
    documents = state.get("documents", [])
    financial_domain = state.get("financial_domain", "general")
    thought_process = state.get("thought_process", [])
    
    # ì£¼ì‹ ë„ë©”ì¸ì´ ì•„ë‹ˆë©´ ìŠ¤í‚µ
    if financial_domain != "stock":
        thought_process.append("ğŸ” íšŒì‚¬ ë¹„êµ ë¶„ì„: ì£¼ì‹ ë„ë©”ì¸ì´ ì•„ë‹ˆë¯€ë¡œ ìŠ¤í‚µ")
        return {
            "company_comparison_data": None,
            "thought_process": thought_process
        }
    
    # ì§ˆë¬¸ì—ì„œ íšŒì‚¬ëª… ì¶”ì¶œ
    company_extraction_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ íšŒì‚¬ëª…ì´ë‚˜ ê¸°ì—…ëª…ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

íšŒì‚¬ëª…ì´ë‚˜ ê¸°ì—…ëª…ì´ ìˆìœ¼ë©´ ê·¸ ì´ë¦„ë§Œ ë‹µë³€í•˜ê³ , ì—†ìœ¼ë©´ "ì—†ìŒ"ì´ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
ë‹µë³€ í˜•ì‹: íšŒì‚¬ëª…ë§Œ (ì˜ˆ: "ì‚¼ì„±ì „ì", "ì• í”Œ", "ì—†ìŒ"): """
    
    try:
        response = llm.invoke(company_extraction_prompt)
        company_name = response.content.strip() if hasattr(response, 'content') else "ì—†ìŒ"
        
        if "ì—†ìŒ" in company_name or len(company_name) < 2:
            thought_process.append("ğŸ” íšŒì‚¬ ë¹„êµ ë¶„ì„: íšŒì‚¬ëª… ì¶”ì¶œ ì‹¤íŒ¨")
            return {
                "company_comparison_data": None,
                "thought_process": thought_process
            }
        
        thought_process.append(f"ğŸ” íšŒì‚¬ëª… ì¶”ì¶œ: {company_name}")
    except Exception:
        thought_process.append("ğŸ” íšŒì‚¬ ë¹„êµ ë¶„ì„: íšŒì‚¬ëª… ì¶”ì¶œ ì˜¤ë¥˜")
        return {
            "company_comparison_data": None,
            "thought_process": thought_process
        }
    
    # ë¬¸ì„œì—ì„œ íšŒì‚¬ ì •ë³´ ì¶”ì¶œ
    documents_text = "\n\n".join([f"[ë¬¸ì„œ {i+1}]\n{doc[:2000]}..." if len(doc) > 2000 else f"[ë¬¸ì„œ {i+1}]\n{doc}" 
                                  for i, doc in enumerate(documents)])
    
    # íšŒì‚¬ ì •ë³´ ë° ë¹„êµ ë¶„ì„ ìš”ì²­
    comparison_prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {company_name} íšŒì‚¬ì— ëŒ€í•œ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.

[ê²€ìƒ‰ëœ ë¬¸ì„œ]
{documents_text[:5000]}  # ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸°

[ë¶„ì„ ìš”ì²­ì‚¬í•­]
1. {company_name}ì˜ ì—…ì¢…(ì‚°ì—… ë¶„ì•¼)ì„ íŒŒì•…í•˜ì„¸ìš”
2. {company_name}ì˜ ì‹œê°€ì´ì•¡ ê·œëª¨ë¥¼ íŒŒì•…í•˜ì„¸ìš” (ëŒ€í˜•/ì¤‘í˜•/ì†Œí˜•)
3. ê°™ì€ ì—…ì¢…ì—ì„œ ë¹„ìŠ·í•œ ê·œëª¨ì˜ ê²½ìŸì‚¬ë‚˜ ë¹„êµ ê°€ëŠ¥í•œ íšŒì‚¬ 3-5ê°œë¥¼ ì œì‹œí•˜ì„¸ìš”
4. í•´ë‹¹ íšŒì‚¬ë“¤ê³¼ì˜ ë¹„êµ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš” (ì£¼ê°€, ì‹¤ì , PER, PBR ë“±)

ë‹µë³€ì„ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”:
{{
    "target_company": "{company_name}",
    "industry": "ì—…ì¢…ëª…",
    "market_cap_category": "ëŒ€í˜•/ì¤‘í˜•/ì†Œí˜•",
    "similar_companies": ["íšŒì‚¬1", "íšŒì‚¬2", "íšŒì‚¬3"],
    "comparison_insights": "ë¹„êµ ë¶„ì„ ì¸ì‚¬ì´íŠ¸ (ì£¼ê°€, ì‹¤ì , ë°¸ë¥˜ì—ì´ì…˜ ë“± ë¹„êµ)"
}}

ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ê°€ëŠ¥í•œ ë¶€ë¶„ë§Œ ë‹µë³€í•˜ì„¸ìš”: """
    
    try:
        response = llm.invoke(comparison_prompt)
        comparison_text = response.content.strip() if hasattr(response, 'content') else ""
        
        # JSON ì¶”ì¶œ ì‹œë„ (ê°„ë‹¨í•œ íŒŒì‹±)
        import json
        import re
        
        # JSON ë¸”ë¡ ì°¾ê¸°
        json_match = re.search(r'\{.*\}', comparison_text, re.DOTALL)
        if json_match:
            try:
                comparison_data = json.loads(json_match.group())
                thought_process.append(f"âœ… íšŒì‚¬ ë¹„êµ ë¶„ì„ ì™„ë£Œ: {company_name} ({comparison_data.get('industry', 'N/A')} ì—…ì¢…)")
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
                comparison_data = {
                    "target_company": company_name,
                    "industry": "ì •ë³´ ë¶€ì¡±",
                    "market_cap_category": "ì •ë³´ ë¶€ì¡±",
                    "similar_companies": [],
                    "comparison_insights": comparison_text[:500] if comparison_text else "ë¹„êµ ë¶„ì„ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
                thought_process.append(f"âš ï¸ íšŒì‚¬ ë¹„êµ ë¶„ì„: JSON íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ì •ë³´ ì‚¬ìš©")
        else:
            # JSONì´ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ì •ë³´ë§Œ ì‚¬ìš©
            comparison_data = {
                "target_company": company_name,
                "industry": "ì •ë³´ ë¶€ì¡±",
                "market_cap_category": "ì •ë³´ ë¶€ì¡±",
                "similar_companies": [],
                "comparison_insights": comparison_text[:500] if comparison_text else "ë¹„êµ ë¶„ì„ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
            thought_process.append(f"âš ï¸ íšŒì‚¬ ë¹„êµ ë¶„ì„: JSON í˜•ì‹ ì—†ìŒ, í…ìŠ¤íŠ¸ ì •ë³´ ì‚¬ìš©")
        
    except Exception as e:
        comparison_data = {
            "target_company": company_name,
            "industry": "ì˜¤ë¥˜",
            "market_cap_category": "ì˜¤ë¥˜",
            "similar_companies": [],
            "comparison_insights": f"ë¹„êµ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        }
        thought_process.append(f"âš ï¸ íšŒì‚¬ ë¹„êµ ë¶„ì„: ì˜¤ë¥˜ ë°œìƒ")
    
    return {
        "company_comparison_data": comparison_data,
        "thought_process": thought_process
    }


def calculate_confidence(state: AgentState) -> AgentState:
    """
    ì‹ ë¢°ë„ ê³„ì‚° ë…¸ë“œ: ì—¬ëŸ¬ ìš”ì†Œë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°
    ê¸ˆìœµ ì •ë³´ì˜ ì‹ ë¢°ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
    
    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (confidence_score í¬í•¨)
    """
    documents = state.get("documents", [])
    is_relevant = state.get("is_relevant", "no")
    source_agreement = state.get("source_agreement", "low")
    verification_round = state.get("verification_round", 0)
    loop_count = state.get("loop_count", 0)
    thought_process = state.get("thought_process", [])
    
    # ì‹ ë¢°ë„ ê³„ì‚° ë¡œì§
    base_confidence = 0.4  # ê¸°ë³¸ ì‹ ë¢°ë„
    
    # ê´€ë ¨ì„± ì ìˆ˜ (0.3)
    if is_relevant == "yes":
        base_confidence += 0.3
    elif is_relevant == "partial":
        base_confidence += 0.15
    
    # ì†ŒìŠ¤ ì¼ì¹˜ë„ ì ìˆ˜ (0.2)
    if source_agreement == "high":
        base_confidence += 0.2
    elif source_agreement == "medium":
        base_confidence += 0.1
    
    # ê²€ì¦ ë¼ìš´ë“œ ìˆ˜ (0.1) - ë” ë§ì´ ê²€ì¦í• ìˆ˜ë¡ ì‹ ë¢°ë„ ì¦ê°€
    base_confidence += min(verification_round * 0.05, 0.1)
    
    # ë¬¸ì„œ ìˆ˜ (0.1) - ì¶©ë¶„í•œ ì†ŒìŠ¤ê°€ ìˆì„ìˆ˜ë¡ ì‹ ë¢°ë„ ì¦ê°€
    if len(documents) >= 5:
        base_confidence += 0.1
    elif len(documents) >= 3:
        base_confidence += 0.07
    elif len(documents) >= 2:
        base_confidence += 0.03
    
    # ë„ˆë¬´ ë§ì€ ë£¨í”„ëŠ” ì˜¤íˆë ¤ ì‹ ë¢°ë„ ê°ì†Œ (0.1 íŒ¨ë„í‹°)
    if loop_count > 5:
        base_confidence -= 0.1
    
    confidence_score = max(0.0, min(base_confidence, 1.0))
    
    thought_process.append(f"ğŸ“Š ì‹ ë¢°ë„ ê³„ì‚°: {confidence_score:.2f} (ê´€ë ¨ì„±: {is_relevant}, ì†ŒìŠ¤ì¼ì¹˜: {source_agreement}, ê²€ì¦: {verification_round}ë¼ìš´ë“œ)")
    
    return {
        "confidence_score": confidence_score,
        "thought_process": thought_process
    }


def load_pdf_to_vectorstore(file_path: str, persist_directory: str = "./chroma_db"):
    """
    PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìª¼ê°œê³  ChromaDBì— ì €ì¥í•œ í›„ Retrieverë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        file_path: PDF íŒŒì¼ ê²½ë¡œ
        persist_directory: ChromaDB ì €ì¥ ë””ë ‰í† ë¦¬
        
    Returns:
        Retriever ê°ì²´
    """
    # PDF ë¡œë“œ
    loader = PyPDFLoader(file_path)
    documents = loader.load()
    
    # í…ìŠ¤íŠ¸ ë¶„í•  (Split)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    splits = text_splitter.split_documents(documents)
    
    # ChromaDB ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory=persist_directory
    )
    
    # Retriever ìƒì„± ë° ë°˜í™˜
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3}
    )
    
    print(f"ë¬¸ì„œê°€ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {len(splits)}ê°œ ì²­í¬")
    return retriever

